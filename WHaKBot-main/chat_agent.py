import streamlit as st
from langchain_core.messages import AIMessage
from main import set_environment_variables
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_community.tools.tavily_search import TavilySearchResults
from tools import generate_image, generate_image2, get_data, SQL_RAG, get_history
from typing import Annotated, Literal, TypedDict
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import END, START, StateGraph, MessagesState
from langgraph.prebuilt import ToolNode
from langgraph.graph.message import AnyMessage, add_messages
from langchain_core.runnables import Runnable, RunnableConfig
import streamlit as st
from langchain_openai import ChatOpenAI
from langgraph.prebuilt import tools_condition
from sqlconnector import open_connection, insert_data, delete_all, delete_table, close_connection
import os

from tools.image_gen2 import api_key


class State(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]

class Assistant:
    def __init__(self, runnable: Runnable):
        self.runnable = runnable

    def __call__(self, state: MessagesState, config: RunnableConfig):
        while True:

            state = {**state}
            result = self.runnable.invoke(state)
            # If the LLM happens to return an empty response, we will re-prompt it
            # for an actual response.
            if not result.tool_calls and (
                    not result.content
                    or isinstance(result.content, list)
                    and not result.content[0].get("text")
            ):
                messages = state["messages"] + [("user", "Respond with a real output.")]
                state = {**state, "messages": messages}
            else:
                break
        return {"messages": result}



def print_event(event: dict, max_length=1500):
    current_state = event.get("dialog_state")
    if current_state:
        st.write("Currently in: ", current_state[-1])
    message = event.get("messages")
    if message:
        if isinstance(message, list):
            message = message[-1]
        msg_repr = message.pretty_repr(html=True)
        if len(msg_repr) > max_length:
            msg_repr = msg_repr[:max_length] + " ... (truncated)"
        st.write(msg_repr)




st.title("WHaKBot")

#Set environment variables
set_environment_variables()

SCRIPT_AGENT_NAME = "script_agent"
TAVILY_TOOL = TavilySearchResults(max_results=10, tavily_api_key=os.environ['TAVILY_API_KEY'])
tools = [TAVILY_TOOL, generate_image, generate_image2, get_data, SQL_RAG, get_history]
tool_node = ToolNode(tools)

# Set OpenAI API key from Streamlit secrets


primary_assistant_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            """"
You are a helpful and loyal agent with access to many tools.

If you need to pull information from a different conversation session, use the SQL_RAG tool. Otherwise, please use the current memory.
The SQL_RAG tool can also be useful in scenarios where a question appears without much context. Assume that the context might've been provided in a different conversation first,
before defaulting to a custom response.

If you are asked to generate an image, use the generate_image tool. If you use this tool, be sure to only output the url generated by it and nothing else. If this image fails, attempt
to use the image_gen_flux tool.

If you are asked how to do something, or information on something use your TAVILY_SEARCH_TOOL to search for urls related to the topic, please summarize the contents instead of returning the raw output.
        """
,
        ),
        ("placeholder", "{messages}"),
    ]
)


llm = ChatOpenAI(model="gpt-3.5-turbo-0125", temperature=1, api_key = os.environ["OPENAI_API_KEY"])


model = primary_assistant_prompt | llm.bind_tools(tools)


workflow = StateGraph(MessagesState)

# Define the two nodes we will cycle between
workflow.add_node("agent", Assistant(model))
workflow.add_node("tools", tool_node)

# Set the entrypoint as `agent`
# This means that this node is the first one called
workflow.add_edge(START, "agent")

# We now add a conditional edge
workflow.add_conditional_edges(
    # First, we define the start node. We use `agent`.
    # This means these are the edges taken after the `agent` node is called.
    "agent",
    # Next, we pass in the function that will determine which node is called next.
    tools_condition,
)
workflow.add_edge("tools", "agent")

app = workflow.compile()

on = st.toggle("Save to database")

# Set a default model
if "openai_model" not in st.session_state:
    st.session_state["openai_model"] = "gpt-3.5-turbo"

if 'printed_messages' not in st.session_state:
    st.session_state.printed_messages = set()


# Initialize chat history
if "messages" not in st.session_state:
    st.session_state.messages = []

# Display chat messages from history on app rerun
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Accept user input
if prompt := st.chat_input("What is up?"):
    # Add user message to chat history
    st.session_state.messages.append({"role": "user", "content": prompt})

    # Display user message in chat message container
    with st.chat_message("user"):
        st.markdown(prompt)

    # Prepare the messages for the assistant model
    messages = st.session_state.messages

    try:
        # Construct messages for the assistant model
        messages = [
            {"role": m["role"], "content": m["content"]}
            for m in st.session_state.messages
        ]

        # Invoke the assistant model
        final_state = app.invoke({"messages": messages})

        # Extract the assistant's response from final_state
        assistant_response = final_state.get("messages")

        if assistant_response:
            # Get the latest AI message
            if isinstance(assistant_response, list):
                assistant_response = assistant_response[-1]  # Get the last message

            # Check if the response is an AIMessage
            if isinstance(assistant_response, AIMessage):
                msg_repr = assistant_response.content  # Access the content of the AIMessage
                # Append only the content to the session state
                st.session_state.messages.append({"role": "assistant", "content": msg_repr})
            else:
                msg_repr = "No valid AI message found."
        else:
            msg_repr = "No messages received."

        # Truncate if necessary
        if len(msg_repr) > 1000:
            msg_repr = msg_repr[:1000] + " ... (truncated)"

        with st.chat_message("assistant"):
            st.markdown(msg_repr)

        if on:
            conn = open_connection()

            user_input = st.session_state.messages[-2]["content"]  # Get the last user message
            assistant_response = st.session_state.messages[-1]["content"]  # Get the last assistant response
            insert_data(conn, user_input, assistant_response)
            close_connection(conn)
            st.write("Successfully uploaded response to database!")


    except Exception as e:
        st.write(f"Error invoking the model: {e}")
