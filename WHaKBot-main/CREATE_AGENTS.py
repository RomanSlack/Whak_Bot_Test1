import time
import SYSTEM_PROMPTS
from main import set_environment_variables
import functools
import operator
import os
from langchain_openai import ChatOpenAI
from SYSTEM_PROMPTS import SCRIPT_GENERATOR_PROMPT
from langgraph.prebuilt import tools_condition  # Add this if it's missing

from main import set_environment_variables
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_community.tools.tavily_search import TavilySearchResults
from tools import calculator, generate_image, research, generate_image2, video_gen, pdf_summary
from typing import Annotated, Literal, TypedDict
from langchain_anthropic import ChatAnthropic
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import END, START, StateGraph, MessagesState
from langgraph.prebuilt import ToolNode
from langgraph.graph.message import AnyMessage, add_messages
#from langchain_core.runnables import Runnable, RunnableConfig
from langchain_core.runnables import Runnable, RunnableConfig
from langchain import HuggingFaceHub, LLMChain
#from langchain_mistralai import ChatMistralAI

class State(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]

class Assistant:
    def __init__(self, runnable: Runnable):
        self.runnable = runnable

    def __call__(self, state: MessagesState, config: RunnableConfig):
        while True:
            state = {**state}
            result = self.runnable.invoke(state)
            if not result.tool_calls and (
                    not result.content
                    or isinstance(result.content, list)
                    and not result.content[0].get("text")
            ):
                messages = state["messages"] + [("user", "Respond with a real output.")]
                state = {**state, "messages": messages}
            else:
                break
        return {"messages": result}


set_environment_variables("WHaK AI")

SCRIPT_AGENT_NAME = "script_agent"
TAVILY_TOOL = TavilySearchResults(max_results=10, tavily_api_key="jIerUWieSJaYUrGSWZ6Fpxry8dftro2G")
tools = [TAVILY_TOOL, calculator, generate_image, research, generate_image2, pdf_summary, video_gen]
tool_node = ToolNode(tools)

primary_assistant_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            """"
You are a helpful and loyal agent with access to many tools. One major one is the create_video tool.
If someone asks you to help them find information on someone, they will give you either an email, username, or name. You must use the dox_fool tool and input ONLY their name, username, or email for the "info" parameter and the type of info (either name, email, or username) for the source. You will get addresses returned to you. Please output only the addresses in this format:
Potential addresses for (name of person):
1. (address 1)
2. (address 2)
...
If someone asks you a math equation, use the calculator tool. For the input for the calculator tool, use only the expression they give you and nothing else. make sure you put the expression in numexpr syntax.
If you are asked to generate an image, use the generate_image tool. If you use this tool, be sure to only output the url generated by it and nothing else.
If you are asked how to do something, or information on something, use your TAVILY_SEARCH_TOOL to search for urls related to the topic, and summarize the contents instead of returning the raw output.
            """,
        ),
        ("placeholder", "{messages}"),
    ]
)

model = primary_assistant_prompt | ChatOpenAI(model="gpt-3.5-turbo-0125", temperature=1).bind_tools(tools)

workflow = StateGraph(MessagesState)
workflow.add_node("agent", Assistant(model))
workflow.add_node("tools", tool_node)
workflow.add_edge(START, "agent")
workflow.add_conditional_edges("agent", tools_condition)
workflow.add_edge("tools", "agent")

chatbot_workflow = workflow.compile(checkpointer=MemorySaver())

# FastAPI setup
from fastapi import FastAPI
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware  # Import CORS middleware

# Define your FastAPI app
app = FastAPI()

# Enable CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],  # Replace with your frontend's address
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Define a Pydantic model for the input
class ChatRequest(BaseModel):
    message: str

# This endpoint handles the interaction with your LangChain model
@app.post("/chatbot")
async def handle_chat_request(chat_request: ChatRequest):
    user_message = chat_request.message
    messages = [{"role": "user", "content": user_message}]
    
    # Invoke the chatbot logic
    final_state = chatbot_workflow.invoke(
        {"messages": messages},
        config={"configurable": {"thread_id": 42}},
    )

    # Debugging: Print the final_state to inspect its structure
    print("Final State:", final_state)

    # Extract the final response
    response_message = extract_response(final_state)

    return {"response": response_message}


def extract_response(state):
    # Ensure 'messages' exists and is not empty
    if "messages" not in state or len(state["messages"]) == 0:
        return "Error: No messages found in the state"

    # Get the last message, which should be an AIMessage or HumanMessage object
    last_message = state["messages"][-1]

    # Access the content attribute of the AIMessage or HumanMessage object
    if hasattr(last_message, 'content'):
        return last_message.content
    else:
        return "Error: Last message does not contain content"

